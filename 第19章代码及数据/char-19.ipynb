{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import jieba\n",
    "\n",
    "\n",
    "class WordToken(object):\n",
    "    def __init__(self):\n",
    "        # 最小起始id号, 保留的用于表示特殊标记\n",
    "        self.START_ID = 4\n",
    "        self.word2id_dict = {}\n",
    "        self.id2word_dict = {}\n",
    "\n",
    "\n",
    "    def load_file_list(self, file_list, min_freq):\n",
    "        \"\"\"\n",
    "        加载样本文件列表，全部切词后统计词频，按词频由高到低排序后顺次编号\n",
    "        并存到self.word2id_dict和self.id2word_dict中\n",
    "        \"\"\"\n",
    "        words_count = {}\n",
    "        for file in file_list:\n",
    "            with open(file, 'r') as file_object:\n",
    "                for line in file_object.readlines():\n",
    "                    line = line.strip()\n",
    "                    seg_list = jieba.cut(line)\n",
    "                    for str in seg_list:\n",
    "                        if str in words_count:\n",
    "                            words_count[str] = words_count[str] + 1\n",
    "                        else:\n",
    "                            words_count[str] = 1\n",
    "\n",
    "        sorted_list = [[v[1], v[0]] for v in words_count.items()]\n",
    "        sorted_list.sort(reverse=True)\n",
    "        for index, item in enumerate(sorted_list):\n",
    "            word = item[1]\n",
    "            if item[0] < min_freq:\n",
    "                break\n",
    "            self.word2id_dict[word] = self.START_ID + index\n",
    "            self.id2word_dict[self.START_ID + index] = word\n",
    "        return index\n",
    "\n",
    "    def word2id(self, word):\n",
    "        if not isinstance(word, str):\n",
    "            print(\"Exception: error word not unicode\")\n",
    "            sys.exit(1)\n",
    "        if word in self.word2id_dict:\n",
    "            return self.word2id_dict[word]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def id2word(self, id):\n",
    "        id = int(id)\n",
    "        if id in self.id2word_dict:\n",
    "            return self.id2word_dict[id]\n",
    "        else:\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_set():\n",
    "    global num_encoder_symbols, num_decoder_symbols\n",
    "    train_set = []\n",
    "    with open('./chatbot/question.txt', 'r') as question_file:\n",
    "        with open('./chatbot/answer.txt', 'r') as answer_file:\n",
    "            while True:\n",
    "                question = question_file.readline()\n",
    "                answer = answer_file.readline()\n",
    "                if question and answer:\n",
    "                    question = question.strip()\n",
    "                    answer = answer.strip()\n",
    "\n",
    "                    question_id_list = get_id_list_from(question)\n",
    "                    answer_id_list = get_id_list_from(answer)\n",
    "                    answer_id_list.append(EOS_ID)\n",
    "                    train_set.append([question_id_list, answer_id_list])\n",
    "                else:\n",
    "                    break\n",
    "    return train_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_id_list_from(sentence):\n",
    "    sentence_id_list = []\n",
    "    seg_list = jieba.cut(sentence)\n",
    "    for str in seg_list:\n",
    "        id = wordToken.word2id(str)\n",
    "        if id:\n",
    "            sentence_id_list.append(wordToken.word2id(str))\n",
    "    return sentence_id_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 3.139 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "#import word_token\n",
    "import jieba\n",
    "wordToken = WordToken()\n",
    "\n",
    "# 放在全局的位置，为了动态算出num_encoder_symbols和num_decoder_symbols\n",
    "max_token_id = wordToken.load_file_list(['./chatbot/question.txt', './chatbot/answer.txt'],10)\n",
    "num_encoder_symbols = max_token_id + 5\n",
    "num_decoder_symbols = max_token_id + 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict():\n",
    "    \"\"\"\n",
    "    预测过程\n",
    "    \"\"\"\n",
    "    with tf.Session() as sess:\n",
    "        encoder_inputs, decoder_inputs, target_weights,outputs, loss, update, saver, targets= get_model(feed_previous=True)\n",
    "        saver.restore(sess, output_dir)\n",
    "        sys.stdout.write(\"> \")\n",
    "        sys.stdout.flush()\n",
    "        input_seq = sys.stdin.readline()\n",
    "        while input_seq:\n",
    "            input_seq = input_seq.strip()\n",
    "            input_id_list = get_id_list_from(input_seq)\n",
    "            if (len(input_id_list)):\n",
    "                sample_encoder_inputs, sample_decoder_inputs, sample_target_weights= seq_to_encoder(' '.join([str(v) for v in input_id_list]))\n",
    "\n",
    "                input_feed = {}\n",
    "                for l in range(input_seq_len):\n",
    "                    input_feed[encoder_inputs[l].name] = sample_encoder_inputs[l]\n",
    "                for l in range(output_seq_len):\n",
    "                    input_feed[decoder_inputs[l].name] = sample_decoder_inputs[l]\n",
    "                    input_feed[target_weights[l].name] = sample_target_weights[l]\n",
    "                input_feed[decoder_inputs[output_seq_len].name]= np.zeros([2], dtype=np.int32)\n",
    "\n",
    "                # 预测输出\n",
    "                outputs_seq = sess.run(outputs, input_feed)\n",
    "                # 因为输出数据每一个是num_decoder_symbols维的\n",
    "                # 因此找到数值最大的那个就是预测的id，就是这里的argmax函数的功能\n",
    "                outputs_seq = [int(np.argmax(logit[0], axis=0)) for logit in outputs_seq]\n",
    "                # 如果是结尾符，那么后面的语句就不输出了\n",
    "                if EOS_ID in outputs_seq:\n",
    "                    outputs_seq = outputs_seq[:outputs_seq.index(EOS_ID)]\n",
    "                outputs_seq = [wordToken.id2word(v) for v in outputs_seq]\n",
    "                print( \" \".join(outputs_seq))\n",
    "            else:\n",
    "                print(\"WARN：词汇不在服务区\")\n",
    "\n",
    "            sys.stdout.write(\"> \")\n",
    "            sys.stdout.flush()\n",
    "            input_seq = sys.stdin.readline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.legacy_seq2seq.python.ops import seq2seq\n",
    "import jieba\n",
    "import random\n",
    "\n",
    "# 输入序列长度\n",
    "input_seq_len = 5\n",
    "# 输出序列长度\n",
    "output_seq_len = 5\n",
    "# 空值填充0\n",
    "PAD_ID = 0\n",
    "# 输出序列起始标记\n",
    "GO_ID = 1\n",
    "# 结尾标记\n",
    "EOS_ID = 2\n",
    "# LSTM神经元size\n",
    "size = 8\n",
    "# 初始学习率\n",
    "init_learning_rate = 0.001\n",
    "# 在样本中出现频率超过这个值才会进入词表\n",
    "min_freq = 10\n",
    "\n",
    "wordToken = WordToken()\n",
    "\n",
    "output_dir='./model/chatbot/demo'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# 放在全局的位置，为了动态算出num_encoder_symbols和num_decoder_symbols\n",
    "max_token_id = wordToken.load_file_list(['./chatbot/question.txt', './chatbot/answer.txt'], min_freq)\n",
    "num_encoder_symbols = max_token_id + 5\n",
    "num_decoder_symbols = max_token_id + 5\n",
    "\n",
    "\n",
    "def get_id_list_from(sentence):\n",
    "    sentence_id_list = []\n",
    "    seg_list = jieba.cut(sentence)\n",
    "    for str in seg_list:\n",
    "        id = wordToken.word2id(str)\n",
    "        if id:\n",
    "            sentence_id_list.append(wordToken.word2id(str))\n",
    "    return sentence_id_list\n",
    "\n",
    "\n",
    "def get_train_set():\n",
    "    global num_encoder_symbols, num_decoder_symbols\n",
    "    train_set = []\n",
    "    with open('./chatbot/question.txt', 'r') as question_file:\n",
    "        with open('./chatbot/answer.txt', 'r') as answer_file:\n",
    "            while True:\n",
    "                question = question_file.readline()\n",
    "                answer = answer_file.readline()\n",
    "                if question and answer:\n",
    "                    question = question.strip()\n",
    "                    answer = answer.strip()\n",
    "\n",
    "                    question_id_list = get_id_list_from(question)\n",
    "                    answer_id_list = get_id_list_from(answer)\n",
    "                    if len(question_id_list) > 0 and len(answer_id_list) > 0:\n",
    "                        answer_id_list.append(EOS_ID)\n",
    "                        train_set.append([question_id_list, answer_id_list])\n",
    "                else:\n",
    "                    break\n",
    "    return train_set\n",
    "\n",
    "\n",
    "def get_samples(train_set, batch_num):\n",
    "    \"\"\"构造样本数据\n",
    "    :return:\n",
    "        encoder_inputs: [array([0, 0], dtype=int32), array([0, 0], dtype=int32), array([5, 5], dtype=int32),\n",
    "                        array([7, 7], dtype=int32), array([9, 9], dtype=int32)]\n",
    "        decoder_inputs: [array([1, 1], dtype=int32), array([11, 11], dtype=int32), array([13, 13], dtype=int32),\n",
    "                        array([15, 15], dtype=int32), array([2, 2], dtype=int32)]\n",
    "    \"\"\"\n",
    "    # train_set = [[[5, 7, 9], [11, 13, 15, EOS_ID]], [[7, 9, 11], [13, 15, 17, EOS_ID]], [[15, 17, 19], [21, 23, 25, EOS_ID]]]\n",
    "    raw_encoder_input = []\n",
    "    raw_decoder_input = []\n",
    "    if batch_num >= len(train_set):\n",
    "        batch_train_set = train_set\n",
    "    else:\n",
    "        random_start = random.randint(0, len(train_set)-batch_num)\n",
    "        batch_train_set = train_set[random_start:random_start+batch_num]\n",
    "    for sample in batch_train_set:\n",
    "        raw_encoder_input.append([PAD_ID] * (input_seq_len - len(sample[0])) + sample[0])\n",
    "        raw_decoder_input.append([GO_ID] + sample[1] + [PAD_ID] * (output_seq_len - len(sample[1]) - 1))\n",
    "\n",
    "    encoder_inputs = []\n",
    "    decoder_inputs = []\n",
    "    target_weights = []\n",
    "\n",
    "    for length_idx in range(input_seq_len):\n",
    "        encoder_inputs.append(np.array([encoder_input[length_idx] for encoder_input in raw_encoder_input], dtype=np.int32))\n",
    "    for length_idx in range(output_seq_len):\n",
    "        decoder_inputs.append(np.array([decoder_input[length_idx] for decoder_input in raw_decoder_input], dtype=np.int32))\n",
    "        target_weights.append(np.array([\n",
    "            0.0 if length_idx == output_seq_len - 1 or decoder_input[length_idx] == PAD_ID else 1.0 for decoder_input in raw_decoder_input\n",
    "        ], dtype=np.float32))\n",
    "    return encoder_inputs, decoder_inputs, target_weights\n",
    "\n",
    "\n",
    "def seq_to_encoder(input_seq):\n",
    "    \"\"\"从输入空格分隔的数字id串，转成预测用的encoder、decoder、target_weight等\n",
    "    \"\"\"\n",
    "    input_seq_array = [int(v) for v in input_seq.split()]\n",
    "    encoder_input = [PAD_ID] * (input_seq_len - len(input_seq_array)) + input_seq_array\n",
    "    decoder_input = [GO_ID] + [PAD_ID] * (output_seq_len - 1)\n",
    "    encoder_inputs = [np.array([v], dtype=np.int32) for v in encoder_input]\n",
    "    decoder_inputs = [np.array([v], dtype=np.int32) for v in decoder_input]\n",
    "    target_weights = [np.array([1.0], dtype=np.float32)] * output_seq_len\n",
    "    return encoder_inputs, decoder_inputs, target_weights\n",
    "\n",
    "\n",
    "def get_model(feed_previous=False):\n",
    "    \"\"\"构造模型\n",
    "    \"\"\"\n",
    "\n",
    "    learning_rate = tf.Variable(float(init_learning_rate), trainable=False, dtype=tf.float32)\n",
    "    learning_rate_decay_op = learning_rate.assign(learning_rate * 0.9)\n",
    "\n",
    "    encoder_inputs = []\n",
    "    decoder_inputs = []\n",
    "    target_weights = []\n",
    "    for i in range(input_seq_len):\n",
    "        encoder_inputs.append(tf.placeholder(tf.int32, shape=[None], name=\"encoder{0}\".format(i)))\n",
    "    for i in range(output_seq_len + 1):\n",
    "        decoder_inputs.append(tf.placeholder(tf.int32, shape=[None], name=\"decoder{0}\".format(i)))\n",
    "    for i in range(output_seq_len):\n",
    "        target_weights.append(tf.placeholder(tf.float32, shape=[None], name=\"weight{0}\".format(i)))\n",
    "\n",
    "    # decoder_inputs左移一个时序作为targets\n",
    "    targets = [decoder_inputs[i + 1] for i in range(output_seq_len)]\n",
    "\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(size)\n",
    "\n",
    "    # 这里输出的状态我们不需要\n",
    "    outputs, _ = seq2seq.embedding_attention_seq2seq(\n",
    "                        encoder_inputs,\n",
    "                        decoder_inputs[:output_seq_len],\n",
    "                        cell,\n",
    "                        num_encoder_symbols=num_encoder_symbols,\n",
    "                        num_decoder_symbols=num_decoder_symbols,\n",
    "                        embedding_size=size,\n",
    "                        output_projection=None,\n",
    "                        feed_previous=feed_previous,\n",
    "                        dtype=tf.float32)\n",
    "\n",
    "    # 计算加权交叉熵损失\n",
    "    loss = seq2seq.sequence_loss(outputs, targets, target_weights)\n",
    "    # 使用自适应优化器\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    # 优化目标：让loss最小化\n",
    "    update = opt.apply_gradients(opt.compute_gradients(loss))\n",
    "    # 模型持久化\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "    return encoder_inputs, decoder_inputs, target_weights, outputs, loss, update, saver, learning_rate_decay_op, learning_rate\n",
    "\n",
    "\n",
    "def train():\n",
    "    \"\"\"\n",
    "    训练过程\n",
    "    \"\"\"\n",
    "    train_set = get_train_set()\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        encoder_inputs, decoder_inputs, target_weights, outputs, loss, update, saver, learning_rate_decay_op, learning_rate = get_model()\n",
    "\n",
    "        # 全部变量初始化\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # 训练很多次迭代，每隔100次打印一次loss，可以看情况直接ctrl+c停止\n",
    "        previous_losses = []\n",
    "        for step in range(10000):\n",
    "            sample_encoder_inputs, sample_decoder_inputs, sample_target_weights = get_samples(train_set, 1000)\n",
    "            input_feed = {}\n",
    "            for l in range(input_seq_len):\n",
    "                input_feed[encoder_inputs[l].name] = sample_encoder_inputs[l]\n",
    "            for l in range(output_seq_len):\n",
    "                input_feed[decoder_inputs[l].name] = sample_decoder_inputs[l]\n",
    "                input_feed[target_weights[l].name] = sample_target_weights[l]\n",
    "            input_feed[decoder_inputs[output_seq_len].name] = np.zeros([len(sample_decoder_inputs[0])], dtype=np.int32)\n",
    "            [loss_ret, _] = sess.run([loss, update], input_feed)\n",
    "            if step % 100 == 0:\n",
    "                print( 'step=', step, 'loss=', loss_ret, 'learning_rate=', learning_rate.eval())\n",
    "\n",
    "                if len(previous_losses) > 5 and loss_ret > max(previous_losses[-5:]):\n",
    "                    sess.run(learning_rate_decay_op)\n",
    "                previous_losses.append(loss_ret)\n",
    "\n",
    "                # 模型持久化\n",
    "                saver.save(sess, output_dir)\n",
    "\n",
    "\n",
    "def predict():\n",
    "    \"\"\"\n",
    "    预测过程\n",
    "    \"\"\"\n",
    "    with tf.Session() as sess:\n",
    "        encoder_inputs, decoder_inputs, target_weights, outputs, loss, update, saver, learning_rate_decay_op, learning_rate = get_model(feed_previous=True)\n",
    "        saver.restore(sess, output_dir)\n",
    "        sys.stdout.write(\"> \")\n",
    "        sys.stdout.flush()\n",
    "        input_seq=input()\n",
    "        while input_seq:\n",
    "            input_seq = input_seq.strip()\n",
    "            input_id_list = get_id_list_from(input_seq)\n",
    "            if (len(input_id_list)):\n",
    "                sample_encoder_inputs, sample_decoder_inputs, sample_target_weights = seq_to_encoder(' '.join([str(v) for v in input_id_list]))\n",
    "\n",
    "                input_feed = {}\n",
    "                for l in range(input_seq_len):\n",
    "                    input_feed[encoder_inputs[l].name] = sample_encoder_inputs[l]\n",
    "                for l in range(output_seq_len):\n",
    "                    input_feed[decoder_inputs[l].name] = sample_decoder_inputs[l]\n",
    "                    input_feed[target_weights[l].name] = sample_target_weights[l]\n",
    "                input_feed[decoder_inputs[output_seq_len].name] = np.zeros([2], dtype=np.int32)\n",
    "\n",
    "                # 预测输出\n",
    "                outputs_seq = sess.run(outputs, input_feed)\n",
    "                # 因为输出数据每一个是num_decoder_symbols维的，因此找到数值最大的那个就是预测的id，就是这里的argmax函数的功能\n",
    "                outputs_seq = [int(np.argmax(logit[0], axis=0)) for logit in outputs_seq]\n",
    "                # 如果是结尾符，那么后面的语句就不输出了\n",
    "                if EOS_ID in outputs_seq:\n",
    "                    outputs_seq = outputs_seq[:outputs_seq.index(EOS_ID)]\n",
    "                outputs_seq = [wordToken.id2word(v) for v in outputs_seq]\n",
    "                print(\" \".join(outputs_seq))\n",
    "            else:\n",
    "                print(\"WARN：词汇不在服务区\")\n",
    "\n",
    "            sys.stdout.write(\"> \")\n",
    "            sys.stdout.flush()\n",
    "            input_seq = input()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tf.reset_default_graph() \n",
    "    train()\n",
    "    tf.reset_default_graph()\n",
    "    predict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
